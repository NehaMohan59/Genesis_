{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain-of-Thought Prompting\n",
    "\n",
    "Chain-of-Thought (CoT) prompting enhances complex reasoning by encouraging the model to break down problems into intermediate reasoning steps. When combined with few-shot prompting, it can significantly improve performance on tasks that require multi-step reasoning before arriving at a response.\n",
    "\n",
    "## Automatic Chain-of-Thought (Auto-CoT)\n",
    "\n",
    "Traditionally, using CoT prompting with demonstrations involves manually crafting diverse and effective examples. This manual effort is time-consuming and can lead to less-than-optimal results. To address this, Zhang et al. (2022) introduced Auto-CoT, an automated approach that minimizes manual involvement. Their method uses the prompt “Let’s think step by step” to generate reasoning chains automatically for demonstrations. However, this automatic process is not immune to errors. To reduce the impact of such mistakes, the approach emphasizes the importance of diverse demonstrations.\n",
    "\n",
    "Auto-CoT operates in two main stages:\n",
    "\n",
    "1. **Question Clustering:** Questions from the dataset are grouped into clusters based on similarity or relevance.\n",
    "2. **Demonstration Sampling:** A representative question from each cluster is selected, and its reasoning chain is generated using Zero-Shot-CoT guided by simple heuristics.\n",
    "\n",
    "\n",
    "## References:\n",
    "\n",
    "* (Wei et al. (2022),)[https://arxiv.org/abs/2201.11903]\n",
    "* (OpenAI Documentation for Prompt Engineering)[https://platform.openai.com/docs/guides/prompt-engineering]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running this code on MyBind.org\n",
    "\n",
    "Note: remember that you will need to **adjust CONFIG** with **proper URL and API_KEY**!\n",
    "\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/GenILab-FAU/prompt-eng/HEAD?urlpath=%2Fdoc%2Ftree%2Fprompt-eng%2Fchain_of_thought.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'llama3.2:latest', 'prompt': '\\nThe odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\\nA: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.\\nThe odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.\\nA: Adding all the odd numbers (17, 19) gives 36. The answer is True.\\nThe odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.\\nA: Adding all the odd numbers (11, 13) gives 24. The answer is True.\\nThe odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.\\nA: Adding all the odd numbers (17, 9, 13) gives 39. The answer is False.\\nNow Add all odd numbers until 200 and let me know. \\nProvide only the answer, no explanation!\\n', 'stream': False, 'options': {'temperature': 1.0, 'num_ctx': 100, 'num_predict': 100}}\n",
      "False\n",
      "Time taken: 5.63s\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## CHAIN-OF-THOUGHT  PROMPTING\n",
    "##\n",
    "\n",
    "from _pipeline import create_payload, model_req\n",
    "\n",
    "\n",
    "#### (1) Adjust the inbounding  Prompt, simulating inbounding requests from users or other systems\n",
    "MESSAGE = \"200\"\n",
    "\n",
    "#### (2) Adjust the Prompt Engineering Technique to be applied, simulating Workflow Templates\n",
    "CHAIN_OF_THOUGHT = \\\n",
    "f\"\"\"\n",
    "The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
    "A: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.\n",
    "The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.\n",
    "A: Adding all the odd numbers (17, 19) gives 36. The answer is True.\n",
    "The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.\n",
    "A: Adding all the odd numbers (11, 13) gives 24. The answer is True.\n",
    "The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.\n",
    "A: Adding all the odd numbers (17, 9, 13) gives 39. The answer is False.\n",
    "Now Add all odd numbers until {MESSAGE} and let me know. \n",
    "Provide only the answer, no explanation!\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = CHAIN_OF_THOUGHT \n",
    "\n",
    "#### (3) Configure the Model request, simulating Workflow Orchestration\n",
    "# Documentation: https://github.com/ollama/ollama/blob/main/docs/api.md\n",
    "payload = create_payload(target=\"ollama\",\n",
    "                         model=\"llama3.2:latest\", \n",
    "                         prompt=PROMPT, \n",
    "                         temperature=1.0, \n",
    "                         num_ctx=100, \n",
    "                         num_predict=100)\n",
    "\n",
    "### YOU DONT NEED TO CONFIGURE ANYTHING ELSE FROM THIS POINT\n",
    "# Send out to the model\n",
    "time, response = model_req(payload=payload)\n",
    "print(response)\n",
    "if time: print(f'Time taken: {time}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'llama3.2:latest', 'prompt': '\\nYou are a study companion AI designed to help students understand advanced concepts step by step. Your task is to explain *how Transformers work in Generative AI* in an easy-to-understand way.\\n\\nFollow this structured reasoning process:\\n\\n1. *Define the Concept*  \\n   - What are Transformers in AI?  \\n   - How do they differ from traditional neural networks?  \\n\\n2. *Explain the Key Components*  \\n   - Self-Attention Mechanism  \\n   - Positional Encoding  \\n   - Multi-Head Attention  \\n   - Feedforward Networks  \\n\\n3. *Describe How They Generate Text*  \\n   - Step-by-step breakdown of how Transformers process input and generate responses.  \\n   - How models like GPT and BERT use Transformers.  \\n\\n4. *Use a Simple Analogy*  \\n   - Explain Transformers using a real-world analogy (e.g., a librarian categorizing books).  \\n\\n5. *Discuss Strengths and Limitations*  \\n   - Why are Transformers so powerful for AI applications?  \\n   - What are the challenges, such as computational cost and biases?  \\n\\n### *Example Thought Process*\\n*User Request:* \"Explain how Transformers work in Generative AI.\"\\n\\n*Step 1 - Define the Concept:*  \\n- Transformers are deep learning models designed to process sequential data efficiently.  \\n- Unlike RNNs, they process entire sequences at once, making them more effective for NLP tasks.  \\n\\n*Step 2 - Explain the Key Components:*  \\n- *Self-Attention Mechanism* allows the model to focus on relevant words in a sentence.  \\n- *Positional Encoding* ensures word order is preserved.  \\n- *Multi-Head Attention* helps the model understand different contextual meanings.  \\n\\n*Step 3 - Describe How They Generate Text:*  \\n- A Transformer takes an input sentence, encodes relationships between words, and predicts the next word iteratively.  \\n- GPT (Generative Pre-trained Transformer) uses a decoder-based Transformer to generate text.  \\n\\n*Step 4 - Use a Simple Analogy:*  \\n- Imagine a librarian organizing books by relevance instead of just alphabetical order.  \\n- The librarian (Transformer) understands book relationships better than a simple sorting system.  \\n\\n*Step 5 - Discuss Strengths and Limitations:*  \\n- Strengths: Handles long-range dependencies, scales well, and improves language understanding.  \\n- Limitations: High computational cost, potential biases in training data.  \\n\\nNow, based on this structure, explain the given Generative AI topic step by step.\\n', 'stream': False, 'options': {'temperature': 0.7, 'num_ctx': 10000, 'num_predict': 40000}}\n",
      "**Step 1 - Define the Concept**\n",
      "\n",
      "Transformers are a type of deep learning model designed to process sequential data efficiently. They differ from traditional neural networks (RNNs) in that they don't rely on recurrent connections to process sequences one step at a time. Instead, Transformers use self-attention mechanisms to weigh the importance of different words or tokens within a sequence.\n",
      "\n",
      "**Step 2 - Explain the Key Components**\n",
      "\n",
      "Transformers consist of three main components:\n",
      "\n",
      "1. **Self-Attention Mechanism**: This mechanism allows the model to focus on relevant words in a sentence. It calculates attention scores between all pairs of tokens and weights them accordingly, effectively highlighting the most important information.\n",
      "2. **Positional Encoding**: To preserve word order, positional encoding is used. This adds a fixed vector to each token based on its position in the sequence, allowing the model to understand the relationships between words that are close together or far apart.\n",
      "3. **Multi-Head Attention**: This helps the model understand different contextual meanings by applying multiple attention heads with different weights and biases. Each head processes the input independently, and the outputs are concatenated and linearly transformed.\n",
      "\n",
      "**Step 3 - Describe How They Generate Text**\n",
      "\n",
      "Transformers process input sentences in a step-by-step manner:\n",
      "\n",
      "1. **Input Encoding**: The input sentence is encoded into a sequence of tokens (e.g., words or subwords).\n",
      "2. **Self-Attention Mechanism**: The model applies self-attention to the input tokens, calculating attention scores and weights.\n",
      "3. **Positional Encoding**: The positional encoding is applied to each token, preserving word order.\n",
      "4. **Multi-Head Attention**: The multi-head attention mechanism processes the input independently for each head.\n",
      "5. **Feedforward Network**: The output from the self-attention mechanism is passed through a feedforward network (FFN) consisting of two linear layers with a ReLU activation function in between.\n",
      "6. **Output Generation**: The final output is generated by applying a softmax activation function to the last FFN layer.\n",
      "\n",
      "**Step 4 - Use a Simple Analogy**\n",
      "\n",
      "Imagine a librarian organizing books on a shelf. Traditional neural networks would be like a simple alphabetical sorting system, where each book is sorted based solely on its title. In contrast, Transformers are like a librarian who understands not only the title but also the relationships between different books (e.g., genre, author, publication date). The self-attention mechanism allows the librarian to weigh the importance of each book and prioritize relevant ones.\n",
      "\n",
      "**Step 5 - Discuss Strengths and Limitations**\n",
      "\n",
      "Transformers have several strengths:\n",
      "\n",
      "* **Handles long-range dependencies**: Transformers can capture complex relationships between tokens that are far apart in the sequence.\n",
      "* **Scales well**: Transformers can process sequences of any length, making them suitable for a wide range of NLP tasks.\n",
      "* **Improves language understanding**: By focusing on relevant words and relationships, Transformers improve language understanding and generation capabilities.\n",
      "\n",
      "However, Transformers also have limitations:\n",
      "\n",
      "* **High computational cost**: Training large Transformer models requires significant computational resources and time.\n",
      "* **Potential biases in training data**: Transformers can inherit biases present in the training data, which may not be representative of the entire population or domain.\n",
      "Time taken: 58.02s\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## CHAIN-OF-THOUGHT  PROMPTING\n",
    "##\n",
    "\n",
    "from _pipeline import create_payload, model_req\n",
    "\n",
    "\n",
    "#### (1) Adjust the inbounding  Prompt, simulating inbounding requests from users or other systems\n",
    "MESSAGE = \"Explain how Transformers work in Generative AI.\"\n",
    "\n",
    "#### (2) Adjust the Prompt Engineering Technique to be applied, simulating Workflow Templates\n",
    "CHAIN_OF_THOUGHT =  \\\n",
    "f\"\"\"\n",
    "You are a study companion AI designed to help students understand advanced concepts step by step. Your task is to explain *how Transformers work in Generative AI* in an easy-to-understand way.\n",
    "\n",
    "Follow this structured reasoning process:\n",
    "\n",
    "1. *Define the Concept*  \n",
    "   - What are Transformers in AI?  \n",
    "   - How do they differ from traditional neural networks?  \n",
    "\n",
    "2. *Explain the Key Components*  \n",
    "   - Self-Attention Mechanism  \n",
    "   - Positional Encoding  \n",
    "   - Multi-Head Attention  \n",
    "   - Feedforward Networks  \n",
    "\n",
    "3. *Describe How They Generate Text*  \n",
    "   - Step-by-step breakdown of how Transformers process input and generate responses.  \n",
    "   - How models like GPT and BERT use Transformers.  \n",
    "\n",
    "4. *Use a Simple Analogy*  \n",
    "   - Explain Transformers using a real-world analogy (e.g., a librarian categorizing books).  \n",
    "\n",
    "5. *Discuss Strengths and Limitations*  \n",
    "   - Why are Transformers so powerful for AI applications?  \n",
    "   - What are the challenges, such as computational cost and biases?  \n",
    "\n",
    "### *Example Thought Process*\n",
    "*User Request:* \"{MESSAGE}\"\n",
    "\n",
    "*Step 1 - Define the Concept:*  \n",
    "- Transformers are deep learning models designed to process sequential data efficiently.  \n",
    "- Unlike RNNs, they process entire sequences at once, making them more effective for NLP tasks.  \n",
    "\n",
    "*Step 2 - Explain the Key Components:*  \n",
    "- *Self-Attention Mechanism* allows the model to focus on relevant words in a sentence.  \n",
    "- *Positional Encoding* ensures word order is preserved.  \n",
    "- *Multi-Head Attention* helps the model understand different contextual meanings.  \n",
    "\n",
    "*Step 3 - Describe How They Generate Text:*  \n",
    "- A Transformer takes an input sentence, encodes relationships between words, and predicts the next word iteratively.  \n",
    "- GPT (Generative Pre-trained Transformer) uses a decoder-based Transformer to generate text.  \n",
    "\n",
    "*Step 4 - Use a Simple Analogy:*  \n",
    "- Imagine a librarian organizing books by relevance instead of just alphabetical order.  \n",
    "- The librarian (Transformer) understands book relationships better than a simple sorting system.  \n",
    "\n",
    "*Step 5 - Discuss Strengths and Limitations:*  \n",
    "- Strengths: Handles long-range dependencies, scales well, and improves language understanding.  \n",
    "- Limitations: High computational cost, potential biases in training data.  \n",
    "\n",
    "Now, based on this structure, explain the given Generative AI topic step by step.\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = CHAIN_OF_THOUGHT \n",
    "\n",
    "#### (3) Configure the Model request, simulating Workflow Orchestration\n",
    "# Documentation: https://github.com/ollama/ollama/blob/main/docs/api.md\n",
    "payload = create_payload(target=\"ollama\",\n",
    "                         model=\"llama3.2:latest\", \n",
    "                         prompt=PROMPT, \n",
    "                         temperature=0.7, \n",
    "                         num_ctx=10000, \n",
    "                         num_predict=40000)\n",
    "\n",
    "### YOU DONT NEED TO CONFIGURE ANYTHING ELSE FROM THIS POINT\n",
    "# Send out to the model\n",
    "time, response = model_req(payload=payload)\n",
    "print(response)\n",
    "if time: print(f'Time taken: {time}s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
